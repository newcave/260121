import base64
import os
from pathlib import Path
from dataclasses import dataclass
from typing import List, Optional
from urllib.parse import quote, urljoin, urlparse

import requests
import streamlit as st
from bs4 import BeautifulSoup
from openai import OpenAI

APP_TITLE = "K-water ì—°êµ¬ë³´ê³ ì„œ ìš”ì•½ & í€´ì¦ˆ ì±—ë´‡"
PERSONA = "ë¬¼ê´€ë¦¬ ì „ë¬¸ K-waterì—°êµ¬ì›"
LOGO_PATH = Path("assets/kwater-ai-lab-logo.svg")
ALIO_SEARCH_URL = (
    "https://www.alio.go.kr/search/searchTotal.do?word=%ED%95%9C%EA%B5%AD%EC%88%98%EC%9E%90%EC%9B%90%EA%B3%B5%EC%82%AC+%EC%97%B0%EA%B5%AC%EB%B3%B4%EA%B3%A0%EC%84%9C"
    "&apbaNm=&targetList=jeonggi%2Csusi%2CinfoCenter%2Cemployment%2Cbid%2Cnotice&attachFileYn=Y&sortType=LATEST"
)
ALIO_ORGAN_LIST_URL = "https://alio.go.kr/item/itemOrganList.do?apbaId=C0221&reportFormRootNo=B1040"


@dataclass
class SourceResult:
    url: str
    text: str
    is_fallback: bool


def clean_text(raw_text: str) -> str:
    cleaned = " ".join(raw_text.split())
    return cleaned.strip()


def fetch_url_text(url: str, timeout: int = 12) -> Optional[str]:
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
    }
    response = requests.get(url, headers=headers, timeout=timeout)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "lxml")
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()
    text = soup.get_text(" ")
    cleaned = clean_text(text)
    if len(cleaned) < 500:
        return None
    return cleaned


def fetch_html(url: str, timeout: int = 12) -> str:
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
    }
    response = requests.get(url, headers=headers, timeout=timeout)
    response.raise_for_status()
    return response.text


@st.cache_data(show_spinner=False)
def fetch_binary(url: str, timeout: int = 20) -> tuple[bytes, str]:
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
    }
    response = requests.get(url, headers=headers, timeout=timeout)
    response.raise_for_status()
    mimetype = response.headers.get("Content-Type", "application/pdf").split(";")[0]
    return response.content, mimetype


def extract_alio_report_links(page_url: str, html: str, max_links: int = 8) -> List[str]:
    soup = BeautifulSoup(html, "lxml")
    base_url = f"{urlparse(page_url).scheme}://{urlparse(page_url).netloc}"
    candidates = []
    for anchor in soup.select("a[href]"):
        href = anchor.get("href", "")
        if any(token in href for token in ("itemDetail.do", "itemDetail", "itemDetailInfo")):
            candidates.append(urljoin(base_url, href))
        if len(candidates) >= max_links:
            break
    seen = set()
    deduped = []
    for link in candidates:
        if link not in seen:
            seen.add(link)
            deduped.append(link)
    return deduped


def extract_pdf_links(page_url: str, html: str, max_links: int = 6) -> List[str]:
    soup = BeautifulSoup(html, "lxml")
    base_url = f"{urlparse(page_url).scheme}://{urlparse(page_url).netloc}"
    candidates = []
    for anchor in soup.select("a[href]"):
        href = anchor.get("href", "")
        lower_href = href.lower()
        if ".pdf" in lower_href or "filedown" in lower_href or "download" in lower_href:
            candidates.append(urljoin(base_url, href))
        if len(candidates) >= max_links:
            break
    seen = set()
    deduped = []
    for link in candidates:
        if link not in seen:
            seen.add(link)
            deduped.append(link)
    return deduped


def looks_like_alio_listing(url: str) -> bool:
    return "searchTotal.do" in url or "itemOrganList.do" in url


def search_kwater_reports(query: str, max_results: int = 5) -> List[str]:
    search_url = f"https://duckduckgo.com/html/?q={quote(query)}"
    response = requests.get(search_url, timeout=12)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "lxml")
    results = []
    for link in soup.select("a.result__a"):
        href = link.get("href")
        if href and href.startswith("http"):
            results.append(href)
        if len(results) >= max_results:
            break
    return results


def get_source_text(primary_url: str, fallback_query: str) -> Optional[SourceResult]:
    if looks_like_alio_listing(primary_url):
        try:
            html = fetch_html(primary_url)
            candidates = extract_alio_report_links(primary_url, html)
        except requests.RequestException:
            candidates = []
        for candidate in candidates:
            try:
                text = fetch_url_text(candidate)
            except requests.RequestException:
                continue
            if text:
                return SourceResult(url=candidate, text=text, is_fallback=False)

    try:
        text = fetch_url_text(primary_url)
        if text:
            return SourceResult(url=primary_url, text=text, is_fallback=False)
    except requests.RequestException:
        text = None

    try:
        candidates = search_kwater_reports(fallback_query)
    except requests.RequestException:
        return None

    for candidate in candidates:
        try:
            text = fetch_url_text(candidate)
        except requests.RequestException:
            continue
        if text:
            return SourceResult(url=candidate, text=text, is_fallback=True)
    return None


def set_source_state(source: SourceResult) -> None:
    st.session_state.report_text = source.text
    st.session_state.source_url = source.url
    st.session_state.pdf_links = []
    try:
        html = fetch_html(source.url)
        st.session_state.pdf_links = extract_pdf_links(source.url, html)
    except requests.RequestException:
        st.session_state.pdf_links = []


def get_openai_client(api_key: str) -> OpenAI:
    return OpenAI(api_key=api_key)


def get_secret_value(key: str) -> Optional[str]:
    try:
        return st.secrets.get(key)
    except Exception:
        return None


def build_summary_prompt(text: str, language: str, max_bullets: int) -> List[dict]:
    return [
        {
            "role": "system",
            "content": (
                f"ë‹¹ì‹ ì€ {PERSONA}ì…ë‹ˆë‹¤. ë‹¤ìŒ ë³´ê³ ì„œë¥¼ {language}ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. "
                f"í•µì‹¬ ìš”ì  {max_bullets}ê°œë¥¼ ë¶ˆë¦¿ìœ¼ë¡œ ì œê³µí•˜ê³ , ë§ˆì§€ë§‰ì— ì •ì±…/í˜„ì—… ì ìš© í¬ì¸íŠ¸ë¥¼ 1ì¤„ë¡œ ë§ë¶™ì´ì„¸ìš”."
            ),
        },
        {"role": "user", "content": text},
    ]


def build_quiz_prompt(text: str, language: str, question_count: int) -> List[dict]:
    return [
        {
            "role": "system",
            "content": (
                f"ë‹¹ì‹ ì€ {PERSONA}ì…ë‹ˆë‹¤. ë‹¤ìŒ ë³´ê³ ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ {language}ë¡œ í€´ì¦ˆë¥¼ ë§Œë“œì„¸ìš”. "
                f"í€´ì¦ˆëŠ” ì´ {question_count}ë¬¸í•­ì´ë©°, ê° ë¬¸í•­ì€ ì§ˆë¬¸ê³¼ ê°„ë‹¨í•œ ì •ë‹µ/í•´ì„¤ì„ í¬í•¨í•©ë‹ˆë‹¤."
            ),
        },
        {"role": "user", "content": text},
    ]


def call_openai(client: OpenAI, model: str, messages: List[dict]) -> str:
    response = client.chat.completions.create(model=model, messages=messages)
    return response.choices[0].message.content.strip()


st.set_page_config(page_title=APP_TITLE, page_icon="ğŸ’§", layout="wide")

with st.sidebar:
    if LOGO_PATH.exists():
        try:
            svg_text = LOGO_PATH.read_text(encoding="utf-8")
            encoded = base64.b64encode(svg_text.encode("utf-8")).decode("utf-8")
            st.markdown(
                f'<img src="data:image/svg+xml;base64,{encoded}" style="width:100%; height:auto;" />',
                unsafe_allow_html=True,
            )
        except OSError:
            st.markdown("**K-water AI Lab**")
    else:
        st.markdown("**K-water AI Lab**")
    st.markdown("### ì„¤ì •")
    api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        value=get_secret_value("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY", ""),
        help="Streamlit Cloudì—ì„œëŠ” Secretsì— ì €ì¥ëœ í‚¤ë¥¼ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.",
    )
    model = st.text_input("ëª¨ë¸", value="gpt-4o-mini")
    language = st.selectbox("ì¶œë ¥ ì–¸ì–´", ["í•œêµ­ì–´", "ì˜ì–´"], index=0)
    max_bullets = st.slider("ìš”ì•½ ë¶ˆë¦¿ ê°œìˆ˜", min_value=3, max_value=10, value=5)
    question_count = st.slider("í€´ì¦ˆ ë¬¸í•­ ìˆ˜", min_value=3, max_value=8, value=5)

st.title(APP_TITLE)

col_left, col_right = st.columns([2, 1])

with col_left:
    st.subheader("ë³´ê³ ì„œ ë¶ˆëŸ¬ì˜¤ê¸°")
    st.markdown("ê³µì‹ ë³´ê³ ì„œ ê²€ìƒ‰ í˜ì´ì§€ ë˜ëŠ” ê²€ìƒ‰ ê²°ê³¼ URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    quick_link_col1, quick_link_col2 = st.columns(2)
    with quick_link_col1:
        if st.button("ALIO ë³´ê³ ì„œ ê²€ìƒ‰", use_container_width=True):
            st.session_state.alio_url = ALIO_ORGAN_LIST_URL
    with quick_link_col2:
        if st.button("ALIO í†µí•©ê²€ìƒ‰ ì˜ˆì‹œ", use_container_width=True):
            st.session_state.alio_url = ALIO_SEARCH_URL
    alio_url = st.text_input(
        "ALIO ë³´ê³ ì„œ URL",
        value=st.session_state.get("alio_url", ""),
        placeholder=ALIO_ORGAN_LIST_URL,
        key="alio_url",
    )
    fallback_query = st.text_input(
        "ëŒ€ì²´ ê²€ìƒ‰ ì¿¼ë¦¬",
        value="K-water ì—°êµ¬ë³´ê³ ì„œ ìƒì‚°ë³´ê³ ì„œ ë…¼ë¬¸ ë¬¼ê´€ë¦¬",
        help="ALIO ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ ì¸í„°ë„·ì—ì„œ ì¶”ê°€ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
    )
    load_button = st.button("ë³´ê³ ì„œ ë¶ˆëŸ¬ì˜¤ê¸°", type="primary")

with col_right:
    st.subheader("ì§„í–‰ ìƒíƒœ")
    status_box = st.empty()
    source_box = st.empty()

if "report_text" not in st.session_state:
    st.session_state.report_text = ""
if "source_url" not in st.session_state:
    st.session_state.source_url = ""
if "summary" not in st.session_state:
    st.session_state.summary = ""
if "quiz" not in st.session_state:
    st.session_state.quiz = ""
if "messages" not in st.session_state:
    st.session_state.messages = []
if "pdf_links" not in st.session_state:
    st.session_state.pdf_links = []
if "alio_candidates" not in st.session_state:
    st.session_state.alio_candidates = []

if load_button:
    if not alio_url:
        status_box.warning("ALIO ë³´ê³ ì„œ URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        status_box.info("ë³´ê³ ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
        st.session_state.alio_candidates = []
        if looks_like_alio_listing(alio_url):
            try:
                listing_html = fetch_html(alio_url)
                st.session_state.alio_candidates = extract_alio_report_links(alio_url, listing_html)
            except requests.RequestException:
                st.session_state.alio_candidates = []
        source = get_source_text(alio_url, fallback_query)
        if not source:
            if st.session_state.alio_candidates:
                status_box.warning("ë³´ê³ ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ëª©ë¡ì—ì„œ ë³´ê³ ì„œë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")
            else:
                status_box.error("ë³´ê³ ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. URL ë˜ëŠ” ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            set_source_state(source)
            fallback_label = "(ëŒ€ì²´ ê²€ìƒ‰ ê²°ê³¼)" if source.is_fallback else "(ALIO ì›ë¬¸)"
            status_box.success(f"ë³´ê³ ì„œ ë¡œë”© ì™„ë£Œ {fallback_label}")
            source_box.markdown(f"**ì‚¬ìš©í•œ ì†ŒìŠ¤:** {source.url}")

st.divider()

st.subheader("ë³´ê³ ì„œ ëª©ë¡")
if st.session_state.alio_candidates:
    selected_url = st.selectbox(
        "ëª©ë¡ì—ì„œ ë³´ê³ ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”.",
        st.session_state.alio_candidates,
        format_func=lambda url: url.replace("https://", ""),
    )
    if st.button("ì„ íƒí•œ ë³´ê³ ì„œ ë¶ˆëŸ¬ì˜¤ê¸°"):
        status_box.info("ì„ íƒí•œ ë³´ê³ ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
        st.session_state.alio_url = selected_url
        source = get_source_text(selected_url, fallback_query)
        if not source:
            status_box.error("ì„ íƒí•œ ë³´ê³ ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í•­ëª©ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.")
        else:
            set_source_state(source)
            fallback_label = "(ëŒ€ì²´ ê²€ìƒ‰ ê²°ê³¼)" if source.is_fallback else "(ALIO ì›ë¬¸)"
            status_box.success(f"ë³´ê³ ì„œ ë¡œë”© ì™„ë£Œ {fallback_label}")
            source_box.markdown(f"**ì‚¬ìš©í•œ ì†ŒìŠ¤:** {source.url}")
else:
    st.info("ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ALIO ê²€ìƒ‰ ê²°ê³¼ URLì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

st.divider()

st.subheader("PDF ë‹¤ìš´ë¡œë“œ")
if st.session_state.pdf_links:
    st.caption("ALIO í˜ì´ì§€ì—ì„œ PDF ë§í¬ë¥¼ ë°œê²¬í•˜ë©´ ë°”ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    for idx, link in enumerate(st.session_state.pdf_links, start=1):
        filename = Path(urlparse(link).path).name or f"report_{idx}.pdf"
        try:
            data, mimetype = fetch_binary(link)
            st.download_button(
                label=f"PDF ë‹¤ìš´ë¡œë“œ {idx}",
                data=data,
                file_name=filename,
                mime=mimetype,
            )
            st.markdown(f"[ì›ë¬¸ ë§í¬]({link})")
        except requests.RequestException:
            st.warning(f"PDFë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {link}")
else:
    st.info("ë³´ê³ ì„œì—ì„œ PDF ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í˜ì´ì§€ë¥¼ ì‹œë„í•´ ì£¼ì„¸ìš”.")

st.divider()

st.subheader("ìš”ì•½ ìƒì„±")
if st.button("ìš”ì•½ ë§Œë“¤ê¸°"):
    if not api_key:
        st.warning("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    elif not st.session_state.report_text:
        st.warning("ë¨¼ì € ë³´ê³ ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”.")
    else:
        with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
            client = get_openai_client(api_key)
            prompt = build_summary_prompt(st.session_state.report_text, language, max_bullets)
            st.session_state.summary = call_openai(client, model, prompt)

if st.session_state.summary:
    st.markdown(st.session_state.summary)

st.divider()

st.subheader("í€´ì¦ˆ ì±—ë´‡")
if st.button("í€´ì¦ˆ ë§Œë“¤ê¸°"):
    if not api_key:
        st.warning("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    elif not st.session_state.report_text:
        st.warning("ë¨¼ì € ë³´ê³ ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”.")
    else:
        with st.spinner("í€´ì¦ˆ ìƒì„± ì¤‘..."):
            client = get_openai_client(api_key)
            prompt = build_quiz_prompt(st.session_state.report_text, language, question_count)
            st.session_state.quiz = call_openai(client, model, prompt)
            st.session_state.messages = []

if st.session_state.quiz:
    st.markdown(st.session_state.quiz)

st.markdown("### ì±—ë´‡ê³¼ ëŒ€í™”")
user_message = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
if user_message:
    if not api_key:
        st.warning("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    elif not st.session_state.report_text:
        st.warning("ë¨¼ì € ë³´ê³ ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”.")
    else:
        st.session_state.messages.append({"role": "user", "content": user_message})
        with st.spinner("ë‹µë³€ ì‘ì„± ì¤‘..."):
            client = get_openai_client(api_key)
            system_prompt = (
                f"ë‹¹ì‹ ì€ {PERSONA}ì…ë‹ˆë‹¤. ë³´ê³ ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. "
                "ì •í™•í•˜ê³  ì‹¤ë¬´ì ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤."
            )
            messages = [{"role": "system", "content": system_prompt}]
            messages.extend(st.session_state.messages)
            reply = call_openai(client, model, messages)
            st.session_state.messages.append({"role": "assistant", "content": reply})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
